{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b77679b",
   "metadata": {},
   "source": [
    "# VA Patronage Data Freshness Monitor (Modularized)\n",
    "\n",
    "**Purpose:** Daily monitoring job for VA patronage modular pipeline data freshness  \n",
    "**Alert Mechanism:** Notebook fails if processing gaps detected, triggering Databricks email notifications  \n",
    "**Schedule:** Daily automated execution via Databricks Jobs  \n",
    "**Time Zone:** All timestamps and comparisons are in UTC to ensure consistency across data sources and processing times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa2cca",
   "metadata": {},
   "source": [
    "## 1. Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d87409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules and initialize modular pipeline configuration\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Optional, Dict, Any, List\n",
    "import sys\n",
    "\n",
    "from databricks.sdk.runtime import dbutils\n",
    "\n",
    "# Databricks: add the project root so `import patronage_modularized` works\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "parts = notebook_path.strip(\"/\").split(\"/\")\n",
    "if parts[0] == \"Workspace\":\n",
    "    base = \"/\" + \"/\".join(parts[:4])\n",
    "else:\n",
    "    base = \"/Workspace/\" + \"/\".join(parts[:3])\n",
    "\n",
    "if base not in sys.path:\n",
    "    sys.path.insert(0, base)\n",
    "\n",
    "from patronage_modularized import config as pipeline\n",
    "import patronage_modularized.discovery as discovery\n",
    "\n",
    "TARGET_TABLE_NAME = pipeline.PATRONAGE_TABLE_NAME\n",
    "PIPELINE_CONFIG = pipeline.PIPELINE_CONFIG\n",
    "\n",
    "# Enable verbose logging to see DEBUG messages\n",
    "pipeline.LOGGING_VERBOSE = True\n",
    "\n",
    "pipeline.log_message('VA Patronage Data Freshness Monitor (Modularized)')\n",
    "pipeline.log_message('=' * 50)\n",
    "pipeline.log_message(f\"Execution Time (UTC): {datetime.now(timezone.utc).strftime(pipeline.PY_DATETIME_FORMAT)} UTC\")\n",
    "pipeline.log_message(f'Target Table: {TARGET_TABLE_NAME}')\n",
    "pipeline.log_message('Purpose: Daily gap detection with automated alerts (UTC aligned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bac276",
   "metadata": {},
   "source": [
    "## 2. Data Freshness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further optimized & modularized data freshness monitoring\n",
    "\n",
    "def _normalize_dbutils_dir(path: str) -> str:\n",
    "    \"\"\"Normalize a directory path for use with dbutils.fs.ls.\n",
    "\n",
    "    dbutils.fs.* expects DBFS URIs like `dbfs:/...` (or certain mount paths).\n",
    "    This normalizes common variants used across notebooks/modules.\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        return path\n",
    "\n",
    "    # Fix common typo-style path: dbfs/mnt/... -> dbfs:/mnt/...\n",
    "    if path.startswith('dbfs/mnt/'):\n",
    "        return 'dbfs:/' + path[len('dbfs/'):]\n",
    "\n",
    "    # If given a mount path like /mnt/... ensure dbfs: prefix\n",
    "    if path.startswith('/mnt/'):\n",
    "        return 'dbfs:' + path\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_latest_timestamp(config: Dict[str, Any]) -> Optional[datetime]:\n",
    "    \"\"\"Fetch the latest timestamp based on the source configuration.\"\"\"\n",
    "    source_type = config.get('type')\n",
    "\n",
    "    if source_type == 'table':\n",
    "        query = f\"\"\"\n",
    "            SELECT MAX(SDP_Event_Created_Timestamp) as latest_timestamp\n",
    "            FROM {TARGET_TABLE_NAME}\n",
    "            WHERE Batch_CD = '{config['batch_cd']}' AND RecordStatus = true\n",
    "        \"\"\"\n",
    "        result = spark.sql(query).collect()\n",
    "        return result[0]['latest_timestamp'] if result and result[0]['latest_timestamp'] else None\n",
    "\n",
    "    elif source_type == 'delta':\n",
    "        delta_path = PIPELINE_CONFIG[pipeline.SOURCE_TYPE_SCD]['pt_sources']['delta_table']\n",
    "        pt_table_info = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\").collect()\n",
    "        return pt_table_info[0]['lastModified'] if pt_table_info else None\n",
    "\n",
    "    elif source_type == 'directory':\n",
    "        dmdc_dir = _normalize_dbutils_dir(config.get('path'))\n",
    "        try:\n",
    "            pipeline.log_message(f'   Checking directory: {dmdc_dir}', level='DEBUG')\n",
    "            files = dbutils.fs.ls(dmdc_dir)\n",
    "            pipeline.log_message(f'   Found {len(files)} items in directory', level='DEBUG')\n",
    "\n",
    "            if not files:\n",
    "                pipeline.log_message('   Directory is empty', level='DEBUG')\n",
    "                return None\n",
    "\n",
    "            # Filter out directories, keep only files\n",
    "            files = [f for f in files if not f.path.endswith('/')]\n",
    "            pipeline.log_message(f'   Filtered to {len(files)} files', level='DEBUG')\n",
    "\n",
    "            if not files:\n",
    "                pipeline.log_message('   No files found (all items were directories)', level='DEBUG')\n",
    "                return None\n",
    "\n",
    "            latest_file = max(files, key=lambda f: f.modificationTime)\n",
    "            pipeline.log_message(f'   Latest file: {latest_file.name}', level='DEBUG')\n",
    "\n",
    "            latest_ts = datetime.fromtimestamp(latest_file.modificationTime / 1000, tz=timezone.utc)\n",
    "            return latest_ts\n",
    "        except Exception as e:\n",
    "            pipeline.log_message(f'   Error reading export directory: {str(e)}', level='WARN')\n",
    "            pipeline.log_message(f'   Directory path checked: {dmdc_dir}', level='WARN')\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _add_months_utc(dt: datetime, months: int) -> datetime:\n",
    "    \"\"\"Add calendar months to a UTC datetime, clamping day to month-end when needed.\"\"\"\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    year = dt.year + (dt.month - 1 + months) // 12\n",
    "    month = (dt.month - 1 + months) % 12 + 1\n",
    "\n",
    "    if month == 12:\n",
    "        next_month = datetime(year + 1, 1, 1, tzinfo=timezone.utc)\n",
    "    else:\n",
    "        next_month = datetime(year, month + 1, 1, tzinfo=timezone.utc)\n",
    "    last_day = (next_month - timedelta(days=1)).day\n",
    "\n",
    "    day = min(dt.day, last_day)\n",
    "    return datetime(year, month, day, dt.hour, dt.minute, dt.second, dt.microsecond, tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "def log_freshness_check(source_name: str, config: Dict[str, Any], files: List):\n",
    "    \"\"\"Log data freshness for a single source based on its configuration.\"\"\"\n",
    "    pipeline.log_message(f'\\n{source_name} FILES')\n",
    "    latest_ts = get_latest_timestamp(config)\n",
    "\n",
    "    if latest_ts:\n",
    "        if hasattr(latest_ts, 'tzinfo') and latest_ts.tzinfo is None:\n",
    "            latest_ts = latest_ts.replace(tzinfo=timezone.utc)\n",
    "\n",
    "        if 'get_due_date' in config:\n",
    "            due_date = config['get_due_date'](latest_ts)\n",
    "            if due_date.tzinfo is None:\n",
    "                due_date = due_date.replace(tzinfo=timezone.utc)\n",
    "\n",
    "            status = 'ON TIME' if current_time_utc <= due_date else 'OVERDUE'\n",
    "            pipeline.log_message(f\"   Latest processed: {latest_ts.strftime('%Y-%m-%d %H:%M:%S %Z')} - Due by: {due_date.strftime('%Y-%m-%d %H:%M:%S %Z')} - {status}\")\n",
    "\n",
    "            if status == 'OVERDUE':\n",
    "                overdue_days = (current_time_utc - due_date).days\n",
    "                critical_issues.append(f\"{source_name} is overdue by {overdue_days} days (UTC)\")\n",
    "        else:\n",
    "            days_since = (current_time_utc - latest_ts).days\n",
    "            status = 'ON TIME' if days_since <= config['expected_days'] else 'OVERDUE'\n",
    "            pipeline.log_message(f\"   Latest processed: {latest_ts.strftime('%Y-%m-%d %H:%M:%S %Z')} ({days_since}d ago) - {status}\")\n",
    "\n",
    "            if status == 'OVERDUE':\n",
    "                critical_issues.append(f\"{source_name} is overdue by {days_since} days (UTC)\")\n",
    "\n",
    "        if 'get_next_expected' in config:\n",
    "            next_expected = config['get_next_expected'](current_time_utc)\n",
    "            pipeline.log_message(f'   Next expected: {next_expected}')\n",
    "\n",
    "    else:\n",
    "        pipeline.log_message('   Latest processed: NO DATA')\n",
    "        critical_issues.append(f'{source_name} has no processed data')\n",
    "\n",
    "    if files:\n",
    "        pipeline.log_message(f'   Unprocessed files available: {len(files)}')\n",
    "        if latest_ts:\n",
    "            latest_available_time = max(files, key=lambda x: x[1])[1]\n",
    "            if isinstance(latest_available_time, str):\n",
    "                latest_available_time = datetime.fromisoformat(latest_available_time)\n",
    "            if latest_available_time.tzinfo is None:\n",
    "                latest_available_time = latest_available_time.replace(tzinfo=timezone.utc)\n",
    "\n",
    "            if latest_available_time > latest_ts:\n",
    "                gap_hours = round((latest_available_time - latest_ts).total_seconds() / 3600)\n",
    "                pipeline.log_message(f'   GAP DETECTED: {gap_hours}h between latest available and processed (UTC)')\n",
    "                gaps_detected.append(f'{source_name}: {gap_hours}h processing gap with {len(files)} unprocessed files (UTC)')\n",
    "    else:\n",
    "        pipeline.log_message('   No unprocessed files detected')\n",
    "\n",
    "\n",
    "current_time_utc = datetime.now(timezone.utc)\n",
    "gaps_detected = []\n",
    "critical_issues = []\n",
    "\n",
    "MONITORING_CONFIG = {\n",
    "    'CG': {\n",
    "        'type': 'table',\n",
    "        'batch_cd': pipeline.SOURCE_TYPE_CG,\n",
    "        'expected_days': 1,\n",
    "    },\n",
    "    'SCD': {\n",
    "        'type': 'table',\n",
    "        'batch_cd': pipeline.SOURCE_TYPE_SCD,\n",
    "        'expected_days': 4,\n",
    "        'get_next_expected': lambda today: 'Wednesday' if today.weekday() < 2 else 'Saturday' if today.weekday() < 5 else 'Next Wednesday',\n",
    "    },\n",
    "    'PT Delta Table': {\n",
    "        'type': 'delta',\n",
    "        'get_due_date': lambda latest_ts: _add_months_utc(latest_ts, 1),\n",
    "    },\n",
    "    'DMDC Export': {\n",
    "        'type': 'directory',\n",
    "        'path': pipeline.DMDC_EXPORT_DIR,\n",
    "        'expected_days': 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline.log_message('Data Source Freshness & Gap Analysis (UTC)')\n",
    "pipeline.log_message('=' * 50)\n",
    "\n",
    "try:\n",
    "    unprocessed_files = discovery.discover_unprocessed_files('update')\n",
    "\n",
    "    for source_name, config in MONITORING_CONFIG.items():\n",
    "        try:\n",
    "            if config.get('type') == 'directory':\n",
    "                files = []\n",
    "            else:\n",
    "                files = unprocessed_files.get(config.get('batch_cd', ''), [])\n",
    "\n",
    "            log_freshness_check(source_name, config, files)\n",
    "        except Exception as e:\n",
    "            pipeline.log_message(f'ERROR processing {source_name}: {str(e)}', level='ERROR')\n",
    "            critical_issues.append(f'{source_name} monitoring failed: {str(e)}')\n",
    "\n",
    "    pipeline.log_message(f\"\\nMonitoring completed at {current_time_utc.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    critical_issues.append(f'Monitoring execution failed: {str(e)}')\n",
    "    pipeline.log_message(f'Freshness monitoring failed: {str(e)}')\n",
    "\n",
    "pipeline.log_message('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc42a6b",
   "metadata": {},
   "source": [
    "## 3. Gap Detection Summary & Alert Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive gap analysis and alert decision logic\n",
    "pipeline.log_message('\\nGap Detection Summary')\n",
    "\n",
    "if gaps_detected:\n",
    "    pipeline.log_message(f'PROCESSING GAPS DETECTED: {len(gaps_detected)}')\n",
    "    for i, gap in enumerate(gaps_detected, 1):\n",
    "        pipeline.log_message(f'   {i}. {gap}')\n",
    "else:\n",
    "    pipeline.log_message('No processing gaps detected')\n",
    "\n",
    "if critical_issues:\n",
    "    pipeline.log_message(f'\\nCRITICAL ISSUES DETECTED: {len(critical_issues)}')\n",
    "    for i, issue in enumerate(critical_issues, 1):\n",
    "        pipeline.log_message(f'   {i}. {issue}')\n",
    "else:\n",
    "    pipeline.log_message('\\nNo critical issues detected')\n",
    "\n",
    "total_issues = len(gaps_detected) + len(critical_issues)\n",
    "alert_required = total_issues > 0\n",
    "\n",
    "pipeline.log_message('\\nAlert Decision:')\n",
    "pipeline.log_message(f'   Total Issues: {total_issues}')\n",
    "pipeline.log_message(f\"   Alert Required: {'YES' if alert_required else 'NO'}\")\n",
    "\n",
    "if alert_required:\n",
    "    pipeline.log_message('   Action: Job will fail to trigger team notifications')\n",
    "else:\n",
    "    pipeline.log_message('   Action: Job will complete successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a7cb3",
   "metadata": {},
   "source": [
    "## 4. Forced Failure for Alert Notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force notebook failure if gaps or critical issues detected\n",
    "if alert_required:\n",
    "    pipeline.log_message('\\n' + '=' * 110)\n",
    "    pipeline.log_message('CRITICAL: DATA PROCESSING GAPS DETECTED')\n",
    "    pipeline.log_message('=' * 110)\n",
    "\n",
    "    pipeline.log_message('\\nIssue Summary:')\n",
    "    if gaps_detected:\n",
    "        pipeline.log_message('\\nProcessing Gaps:')\n",
    "        for gap in gaps_detected:\n",
    "            pipeline.log_message(f'   - {gap}')\n",
    "\n",
    "    if critical_issues:\n",
    "        pipeline.log_message('\\nCritical Issues:')\n",
    "        for issue in critical_issues:\n",
    "            pipeline.log_message(f'   - {issue}')\n",
    "\n",
    "    pipeline.log_message('\\nImmediate Actions Required:')\n",
    "    pipeline.log_message('   1. Review unprocessed files in source directories')\n",
    "    pipeline.log_message('   2. Execute patronage pipeline to process pending data')\n",
    "    pipeline.log_message('   3. Investigate any system or data delivery issues')\n",
    "    pipeline.log_message('   4. Verify data source availability and accessibility')\n",
    "\n",
    "    pipeline.log_message(f\"\\nAlert Time: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
    "    pipeline.log_message('Team will be notified via Databricks job failure email')\n",
    "    pipeline.log_message('=' * 110)\n",
    "\n",
    "    raise Exception(\n",
    "        f\"VA Patronage Data Freshness Alert: {len(gaps_detected)} processing gaps and \"\n",
    "        f\"{len(critical_issues)} critical issues detected. Immediate attention required.\"\n",
    "    )\n",
    "else:\n",
    "    pipeline.log_message('\\n' + '=' * 110)\n",
    "    pipeline.log_message('SUCCESS: All data sources are current')\n",
    "    pipeline.log_message('\\nAll systems operating normally:')\n",
    "    pipeline.log_message('   - No processing gaps detected')\n",
    "    pipeline.log_message('   - No critical issues identified')\n",
    "    pipeline.log_message('   - Data freshness within acceptable parameters')\n",
    "    pipeline.log_message(f\"\\nNext monitoring check: {(datetime.now(timezone.utc) + timedelta(days=1)).strftime('%Y-%m-%d')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
