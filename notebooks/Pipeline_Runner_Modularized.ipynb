{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b0e559-3c38-408b-884d-0c0842fdf797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "language": "markdown"
   },
   "source": [
    "# Patronage Pipeline Runner\n",
    "This notebook runs the Patronage data pipeline. Pipeline logic is implemented in the `patronage_modularized` Python package; the notebook provides a simple run interface and validation utilities.\n",
    "\n",
    "## Quick Start (Recommended Run Order)\n",
    "1. **Cell 4 — Imports & Setup**: prepares imports and reloads pipeline modules\n",
    "2. **Cell 5 — (Optional) Logging Verbosity**: set `VERBOSE_LOGGING = True` for detailed logs\n",
    "3. **Cell 6 — Automatic Mode Detection**: selects `update` vs `rebuild` based on the target Delta table state\n",
    "4. **Cell 7 — Execute Pipeline**: runs `orchestrator.run_pipeline(...)` using the selected mode\n",
    "\n",
    "## Processing Modes (Auto-detected)\n",
    "- **rebuild**: Full historical rebuild (used when the target table is missing or empty)\n",
    "- **update**: Incremental processing when the target table already contains data\n",
    "\n",
    "## What happens during UPDATE (high level)\n",
    "- Processes new Caregiver (CG) files\n",
    "- Processes new Service-Connected Disability (SCD) files\n",
    "- Updates PT indicator flags (as configured in the pipeline)\n",
    "- Applies SCD Type 2 merge logic to preserve history (audit trail via active/expired records)\n",
    "\n",
    "## Notes\n",
    "- **Cell 2** is column mapping / lineage documentation (schema reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e4bf77-d6ba-4787-a9b4-3cb519044c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "language": "markdown"
   },
   "source": [
    "# Patronage - Column Mapping\n",
    "\n",
    "## Delta Table Schema Documentation\n",
    "\n",
    "This table documents the complete data lineage for all columns in the Patronage delta table, including source systems, original column names, and any transformations applied.\n",
    "\n",
    "| Delta Table Column Name | Source | Original Column Name | Description of Transformation | Sent to DMDC |\n",
    "|------------------------|--------|---------------------|------------------------------|---------------|\n",
    "| **edipi** | Identity Correlations Delta Table | edipi | Direct mapping from ICN correlation lookup | **Yes** |\n",
    "| **ICN** | Multiple Sources | Caregiver_ICN__c (CG)<br/>ICN (Identity Correlations)<br/>ICN (from participant_id lookup) | **CG Source**: Truncated to first 10 characters from Caregiver_ICN__c<br/>**SCD Source**: Mapped from participant_id via identity correlations<br/>**Seed File**: Truncated to first 10 characters | No |\n",
    "| **Veteran_ICN** | Caregiver Sources Only | Veteran_ICN__c | **CG Source**: Truncated to first 10 characters<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **participant_id** | Multiple Sources | participant_id (Identity Correlations)<br/>PTCPNT_ID (SCD)<br/>PTCPNT_VET_ID (PAI) | **SCD Source**: Direct mapping from PTCPNT_ID<br/>**PAI Source**: Direct mapping from PTCPNT_VET_ID<br/>**CG Source**: Retrieved via ICN lookup from identity correlations | No |\n",
    "| **Batch_CD** | System Generated | N/A | **CG Records**: Hard-coded as \"CG\"<br/>**SCD Records**: Hard-coded as \"SCD\"<br/>**PAI Records**: Hard-coded as \"SCD\" (processed as SCD updates) | **Yes** |\n",
    "| **Applicant_Type** | Caregiver Sources Only | Applicant_Type__c | **CG Source**: Direct mapping<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **Caregiver_Status** | Caregiver Sources Only | Caregiver_Status__c | **CG Source**: Direct mapping<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **SC_Combined_Disability_Percentage** | SCD Sources Only | CMBNED_DEGREE_DSBLTY | **SCD Source**: Zero-padded to 3 digits, empty strings converted to \"000\"<br/>**CG/PAI Sources**: Set to NULL | **Yes** |\n",
    "| **PT_Indicator** | PAI Sources + Default | PT_35_FLAG (PAI)<br/>target_PT_Indicator (existing records) | **PAI Source**: Direct mapping from PT_35_FLAG<br/>**SCD Records**: Defaults to \"N\" for new records, preserves existing values<br/>**CG Records**: Set to NULL | **Yes** |\n",
    "| **Individual_Unemployability** | Not Currently Populated | N/A | Set to NULL for all sources (placeholder for future implementation) | **Yes** |\n",
    "| **Status_Begin_Date** | Multiple Sources | Dispositioned_Date__c (CG)<br/>DSBL_DTR_DT (SCD)<br/>target_Status_Begin_Date (existing) | **CG Source**: Date formatted from Dispositioned_Date__c to YYYYMMDD<br/>**SCD Source**: Uses existing Status_Begin_Date or DSBL_DTR_DT if new record<br/>**Date Format**: Converted from MMddyyyy to yyyyMMdd | **Yes** |\n",
    "| **Status_Last_Update** | Multiple Sources | DSBL_DTR_DT (SCD)<br/>N/A (CG) | **SCD Source**: Direct mapping from DSBL_DTR_DT<br/>**CG Source**: Set to NULL | **Yes** |\n",
    "| **Status_Termination_Date** | Caregiver Sources Only | Benefits_End_Date__c | **CG Source**: Date formatted from Benefits_End_Date__c to YYYYMMDD<br/>**SCD/PAI Sources**: Set to NULL | **Yes** |\n",
    "| **SDP_Event_Created_Timestamp** | File Metadata | _metadata.file_modification_time<br/>CreatedDate (seed) | **All File Sources**: Extracted from file modification timestamp<br/>**Seed File**: Uses configured start datetime<br/>**PAI Delta Table**: Uses current datetime | No |\n",
    "| **filename** | File Metadata + System | _metadata.file_name<br/>Path (seed)<br/>Generated (PAI) | **File Sources**: Extracted from file metadata<br/>**Seed File**: Full file path<br/>**PAI Delta Updates**: Generated description with timestamp | No |\n",
    "| **RecordLastUpdated** | System Generated | N/A | **New Records**: Set to NULL<br/>**Updated Records**: Set to SDP_Event_Created_Timestamp during merge | No |\n",
    "| **RecordStatus** | System Generated | N/A | **Active Records**: Set to TRUE<br/>**Expired Records**: Set to FALSE during SCD Type 2 updates | No |\n",
    "| **sentToDoD** | System Generated | N/A | **New Records**: Set to FALSE<br/>**Expired Records**: Set to TRUE during updates | No |\n",
    "| **change_log** | System Generated | N/A | **New Records**: \"New Record\"<br/>**Updated Records**: Detailed log of field changes with old→new values | No |\n",
    "| **RecordChangeStatus** | System Generated | N/A | **New Records**: \"New Record\"<br/>**Updated Records**: \"Updated Record\"<br/>**Expired Records**: \"Expired Record\" | No |\n",
    "\n",
    "## Data Source Details\n",
    "\n",
    "### Primary Data Sources:\n",
    "1. **Caregiver Events (CG)**: CARMA system CSV files (`caregiverevent*.csv`)\n",
    "2. **Service-Connected Disability (SCD)**: VADIR (`CPIDODIEX_*.csv`)\n",
    "3. **PT Indicator Legacy (PAI)**: Text files (`WRTS*.txt`)\n",
    "4. **PT Indicator Modern (PAI)**: VBA Delta table (`DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT`)\n",
    "5. **Identity Correlations**: MVI Delta table mapping ICNs to EDIPIs and participant IDs\n",
    "6. **Seed Data**: Initial caregiver population CSV file\n",
    "\n",
    "### Key Transformation Patterns:\n",
    "- **ICN Standardization**: All ICNs truncated to 10 characters for consistency\n",
    "- **Date Standardization**: All dates converted to YYYYMMDD string format\n",
    "- **Null Handling**: Explicit NULL assignment for irrelevant fields per source type\n",
    "- **Change Detection**: xxhash64 used for efficient change identification\n",
    "- **Deduplication**: Window functions ensure latest record per unique key combination\n",
    "- **Audit Trail**: Complete change tracking with before/after values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e7a7d3b0-35d9-4dba-972d-d652e4124db3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Patronage Pipeline Technical Diagram"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "mermaid_diagram_clean = r\"\"\"\n",
    "flowchart TD\n",
    "  subgraph \"MVI Sources\"\n",
    "    SRC1[\"MVI Person<br/>SMVIPerson<br/>MVIPersonICN, ICNStatus\"]\n",
    "    SRC2[\"MVI Site Assoc<br/>SMVIPersonSiteAssociation<br/>TreatingFacilityPersonIdentifier, CorrelationModifiedDateTime\"]\n",
    "    SRC3[\"MVI Institution<br/>NDim.MVIInstitution<br/>MVIInstitutionSID, InstitutionCode\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"Identity Correlation\"\n",
    "    ID1[\"Filter + Latest-per-ICN<br/>ActiveMergedIdentifier ∈ {Active, NULL}<br/>Latest person per ICN (calc_IngestionTimestamp)\"]\n",
    "    ID2[\"Join PSA + Institution<br/>Pivot by InstitutionCode\"]\n",
    "    ID3[\"Quarantine duplicates<br/>Ambiguous mappings\"]\n",
    "    ID4[\"Identity lookup columns used downstream<br/>ICN, participant_id, edipi\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"Identity Storage\"\n",
    "    IST1[\"Identity Lookup Delta<br/>IDENTITY_TABLE_NAME\"]\n",
    "    IST2[\"Duplicates Delta<br/>DUP_IDENTITY_TABLE_NAME\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"Mode Detection (Notebook Runner)\"\n",
    "    MD1[\"Target table check<br/>Delta exists? + rowcount > 0\"]\n",
    "    MD2{\"Mode\"}\n",
    "    MD3[\"UPDATE<br/>End boundary: today 00:00 UTC\"]\n",
    "    MD4[\"REBUILD<br/>End boundary: prev month end (UTC)\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"Discovery\"\n",
    "    DISC[\"Per-batch discovery (CG, SCD)<br/>Start: checkpoint (UPDATE) or begin_date (REBUILD)<br/>Select files where start < mod_time < end\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"CG Processing\"\n",
    "    CG0[\"CG seed file<br/>(only if REBUILD and target empty)\"]\n",
    "    CG1[\"Transform + dedupe\"]\n",
    "    CG2[\"Join identity lookup on ICN\"]\n",
    "    CG3[\"Batch_CD='CG'\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"SCD + PT Processing\"\n",
    "    S0[\"For each SCD file discovered\"]\n",
    "    S1[\"Normalize + integrate PT<br/>PT delta always; PT seed only on REBUILD\"]\n",
    "    S2[\"Join identity lookup; drop null ICN\"]\n",
    "    S3[\"Batch_CD='SCD'\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"SCD Type 2 Merge\"\n",
    "    MRG0[\"Load active target<br/>patronage_unified (RecordStatus=True)\"]\n",
    "    MRG1[\"Change detection + business rules\"]\n",
    "    MRG2[\"Prepare inserts + expirations\"]\n",
    "    MRG3[\"Delta merge into patronage_unified<br/>Full history via RecordStatus\"]\n",
    "  end\n",
    "\n",
    "  subgraph \"Scheduled Tasks\"\n",
    "    SCH1[\"EDIPI backfill<br/>Last Friday of month (UTC)\"]\n",
    "    SCH2[\"DMDC export (Wed/Fri)<br/>Window: last&nbsp;checkpoint&nbsp;→&nbsp;today&nbsp;00:00&nbsp;UTC\"]\n",
    "  end\n",
    "\n",
    "  %% Flow\n",
    "  SRC1 & SRC2 & SRC3 --> ID1 --> ID2 --> ID4 --> IST1\n",
    "  ID2 --> ID3 --> IST2\n",
    "\n",
    "  IST1 --> MD1 --> MD2\n",
    "  MD2 -->|UPDATE| MD3 --> DISC\n",
    "  MD2 -->|REBUILD| MD4 --> DISC\n",
    "\n",
    "  DISC -->|CG files| CG0 --> CG1 --> CG2 --> CG3 --> MRG0\n",
    "  DISC -->|SCD files| S0 --> S1 --> S2 --> S3 --> MRG0\n",
    "\n",
    "  MRG0 --> MRG1 --> MRG2 --> MRG3 --> SCH1\n",
    "  MRG3 --> SCH2\n",
    "\n",
    "  %% Styling (reuse existing palette for consistency)\n",
    "  classDef mviSource fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n",
    "  classDef identityEngine fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n",
    "  classDef storage fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n",
    "  classDef modeDetection fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n",
    "  classDef processing fill:#e0f2f1,stroke:#00796b,stroke-width:2px\n",
    "  classDef merge fill:#f9fbe7,stroke:#827717,stroke-width:2px\n",
    "  classDef scheduled fill:#fff8e1,stroke:#ff8f00,stroke-width:2px\n",
    "\n",
    "  class SRC1,SRC2,SRC3 mviSource\n",
    "  class ID1,ID2,ID3,ID4 identityEngine\n",
    "  class IST1,IST2 storage\n",
    "  class MD1,MD2,MD3,MD4 modeDetection\n",
    "  class DISC,CG0,CG1,CG2,CG3,S0,S1,S2,S3 processing\n",
    "  class MRG0,MRG1,MRG2,MRG3 merge\n",
    "  class SCH1,SCH2 scheduled\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class=\\\"mermaid\\\">\n",
    "{mermaid_diagram_clean}\n",
    "</div>\n",
    "<script src=\\\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\\\"></script>\n",
    "<script>\n",
    "  mermaid.initialize({{ startOnLoad: true, theme: 'default', flowchart: {{ useMaxWidth: true, htmlLabels: true }} }});\n",
    "</script>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14acfa7f-fc27-4485-b529-fe2dec147dd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import Libraries and Patronage script file"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from databricks.sdk.runtime import dbutils\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit\n",
    "import sys\n",
    "\n",
    "# IMPORTANT: sys.path must point to the PARENT folder of the package\n",
    "# so Python can resolve `import patronage_modularized`.\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "parts = notebook_path.strip(\"/\").split(\"/\")\n",
    "if parts[0] == \"Workspace\":\n",
    "    base = \"/\" + \"/\".join(parts[:4])\n",
    "else:\n",
    "    base = \"/Workspace/\" + \"/\".join(parts[:3])\n",
    "\n",
    "if base not in sys.path:\n",
    "    sys.path.insert(0, base)\n",
    "\n",
    "# In the modularized pipeline, logging + constants live in config.\n",
    "from patronage_modularized import config as pipeline\n",
    "import patronage_modularized.orchestrator as orchestrator\n",
    "\n",
    "importlib.reload(pipeline)\n",
    "importlib.reload(orchestrator)\n",
    "\n",
    "pipeline.log_message('Pipeline modules imported and reloaded.')\n",
    "pipeline.log_message(f'Target Table Path: {pipeline.PATRONAGE_TABLE_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cc26a3-9a9f-45f5-8974-808ce8c966c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logging Configuration"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Set this to True for detailed, verbose logging, or False for summary logging.\n",
    "VERBOSE_LOGGING = False\n",
    "\n",
    "# Set the verbosity in the pipeline module\n",
    "pipeline.LOGGING_VERBOSE = VERBOSE_LOGGING\n",
    "\n",
    "pipeline.log_message(f\"Logging verbosity set to: {'DETAILED' if VERBOSE_LOGGING else 'SUMMARY'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c623dec-c14a-4ed1-bbd1-4631fa1159d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Detection and Initialization"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUTOMATIC MODE DETECTION & INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "pipeline.log_message(\"Analyzing current table state to determine processing mode...\")\n",
    "\n",
    "try:\n",
    "    # Check if the target Delta table exists and is not empty.\n",
    "    if DeltaTable.isDeltaTable(spark, pipeline.PATRONAGE_TABLE_PATH) and spark.read.format(\"delta\").load(pipeline.PATRONAGE_TABLE_PATH).count() > 0:\n",
    "        processing_mode = \"update\"\n",
    "        pipeline.log_message(f\"Target table '{pipeline.PATRONAGE_TABLE_NAME}' found with data.\")\n",
    "    else:\n",
    "        processing_mode = \"rebuild\"\n",
    "        pipeline.log_message(f\"Target table '{pipeline.PATRONAGE_TABLE_NAME}' is missing or empty.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If any error occurs (e.g., path not found), default to rebuild.\n",
    "    processing_mode = \"rebuild\"\n",
    "    pipeline.log_message(f\"Could not access target table. Defaulting to REBUILD mode. Error: {str(e)}\")\n",
    "\n",
    "pipeline.log_message(f\"Selected Processing Mode: {processing_mode.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fb736f2-7a57-45a8-8421-3c218832ef2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run the Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the Pipeline\n",
    "# Run the pipeline with the determined mode and logging setting\n",
    "try:\n",
    "    pipeline.log_message(\"Starting the VA Patronage Pipeline...\")\n",
    "    orchestrator.run_pipeline(processing_mode, verbose_logging=VERBOSE_LOGGING)\n",
    "    pipeline.log_message(\"Completed processing VA Patronage Pipeline...\")\n",
    "except Exception as e:\n",
    "    pipeline.log_message(f\"Pipeline error: {str(e)}\")\n",
    "    raise  # Re-raise to see full traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d984553-e2c5-4d17-b7af-fc4d8f1d4523",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute the Pipeline"
    },
    "language": "markdown"
   },
   "source": [
    "## File Processing Reconciliation\n",
    "\n",
    "This section validates that all files in blob storage have been processed and recorded in the Delta table. It compares files from the current month (start of month to yesterday) to ensure no files were missed during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5fa59d-3d8b-4ec3-97f3-20faa8d52af9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File Processing Reconciliation"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import col, lit, to_timestamp\n",
    "\n",
    "def reconcile_file_processing(batch_cd, start_date=None, end_date=None, show_details=True):\n",
    "    \"\"\"\n",
    "    Reconcile blob storage files with Delta table processed files.\n",
    "    \n",
    "    Parameters:\n",
    "        batch_cd: 'CG' or 'SCD'\n",
    "        start_date: Start date for comparison (default: start of current month)\n",
    "        end_date: End date for comparison (default: yesterday 23:59:59)\n",
    "        show_details: Display missing files or just summary\n",
    "    \n",
    "    Returns:\n",
    "        Dict with reconciliation results\n",
    "    \"\"\"\n",
    "    pipeline.log_message(f\"FILE RECONCILIATION REPORT: {batch_cd}\")\n",
    "    \n",
    "    # Calculate default date range\n",
    "    if not start_date:\n",
    "        # Start of current month\n",
    "        now = datetime.now(timezone.utc)\n",
    "        start_date = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "    else:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "    \n",
    "    if not end_date:\n",
    "        # Yesterday end of day\n",
    "        now = datetime.now(timezone.utc)\n",
    "        end_date = (now - relativedelta(days=1)).replace(hour=23, minute=59, second=59, microsecond=999999)\n",
    "        \n",
    "    else:\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\").replace(hour=23, minute=59, second=59, tzinfo=timezone.utc)\n",
    "    \n",
    "    pipeline.log_message(f\"Date Range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "    # Get configuration from pipeline\n",
    "    source_config = pipeline.PIPELINE_CONFIG[batch_cd]\n",
    "    source_path = source_config['path']\n",
    "    \n",
    "    # 1. Get all files from blob storage in date range\n",
    "    try:\n",
    "        file_list = dbutils.fs.ls(source_path)\n",
    "        \n",
    "        blob_files = []\n",
    "        for file_info in file_list:\n",
    "            # Convert millisecond timestamp to UTC datetime\n",
    "            mod_time_utc = datetime.fromtimestamp(file_info.modificationTime / 1000, tz=timezone.utc)\n",
    "            \n",
    "            # Apply filename matching logic\n",
    "            is_match = (\n",
    "                (batch_cd == 'SCD' and file_info.name.startswith(source_config['matching_characters'])) or\n",
    "                (batch_cd == 'CG' and file_info.name.lower().endswith('.csv') and 'caregiver' in file_info.name.lower())\n",
    "            )\n",
    "            \n",
    "            # Filter by date range\n",
    "            if is_match and start_date <= mod_time_utc <= end_date:\n",
    "                blob_files.append({\n",
    "                    'path': file_info.path,\n",
    "                    'modification_time': mod_time_utc\n",
    "                })\n",
    "        \n",
    "        pipeline.log_message(f\"Found {len(blob_files):,} files in blob storage ({source_path})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline.log_message(f\"Error accessing blob storage: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Get processed files from Delta table\n",
    "    try:\n",
    "        delta_table = spark.table(pipeline.PATRONAGE_TABLE_NAME)\n",
    "        \n",
    "        # Filter by Batch_CD and date range\n",
    "        processed_files_df = delta_table.filter(\n",
    "            (col(\"Batch_CD\") == batch_cd) &\n",
    "            (col(\"SDP_Event_Created_Timestamp\") >= lit(start_date)) &\n",
    "            (col(\"SDP_Event_Created_Timestamp\") <= lit(end_date))\n",
    "        ).select(\n",
    "            col(\"filename\"),\n",
    "            col(\"SDP_Event_Created_Timestamp\")\n",
    "        ).distinct()\n",
    "        \n",
    "        processed_count = processed_files_df.count()\n",
    "        pipeline.log_message(f\"Found {processed_count:,} distinct files in Delta table\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline.log_message(f\"Error querying Delta table: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Find missing files (in blob but not in Delta)\n",
    "    if len(blob_files) == 0:\n",
    "        pipeline.log_message(f\"\\nWarning: No files found in blob storage for specified date range\")\n",
    "        return {'blob_count': 0, 'delta_count': processed_count, 'missing_count': 0}\n",
    "    \n",
    "    # Create DataFrame from blob files\n",
    "    blob_df = spark.createDataFrame(blob_files)\n",
    "    \n",
    "    # Left anti join to find files in blob but not in Delta\n",
    "    missing_files_df = blob_df.alias(\"blob\").join(\n",
    "        processed_files_df.alias(\"delta\"),\n",
    "        col(\"blob.path\") == col(\"delta.filename\"),\n",
    "        \"left_anti\"\n",
    "    ).select(\n",
    "        col(\"blob.path\").alias(\"filename\"),\n",
    "        col(\"blob.modification_time\")\n",
    "    ).orderBy(\"modification_time\")\n",
    "    \n",
    "    missing_count = missing_files_df.count()\n",
    "    \n",
    "    # 4. Display results\n",
    "    pipeline.log_message(f\"RECONCILIATION SUMMARY:\")\n",
    "    pipeline.log_message(f\"Blob Storage Files:    {len(blob_files):,}\")\n",
    "    pipeline.log_message(f\"Delta Table Records:   {processed_count:,}\")\n",
    "    pipeline.log_message(f\"Missing Files:         {missing_count:,}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        pipeline.log_message(f\"\\nWARNING: {missing_count} file(s) in blob storage not found in Delta table!\")\n",
    "        \n",
    "        if show_details:\n",
    "            pipeline.log_message(f\"\\nMissing File Details:\")\n",
    "            missing_files_df.show(truncate=False)\n",
    "            \n",
    "        pipeline.log_message(f\"Recommendation: Run pipeline in UPDATE mode to process missing files.\")\n",
    "    else:\n",
    "        pipeline.log_message(f\"SUCCESS: All blob storage files for {batch_cd.upper()} have been processed!\")\n",
    "    \n",
    "    pipeline.log_message(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'blob_count': len(blob_files),\n",
    "        'delta_count': processed_count,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_files': missing_files_df if missing_count > 0 else None\n",
    "    }\n",
    "\n",
    "# Execute reconciliation for CG and SCD\n",
    "pipeline.log_message(\"=\"*70)\n",
    "pipeline.log_message(\"  VA PATRONAGE FILE PROCESSING RECONCILIATION\")\n",
    "pipeline.log_message(\"=\"*70)\n",
    "\n",
    "cg_results = reconcile_file_processing('CG', show_details=True)\n",
    "scd_results = reconcile_file_processing('SCD', show_details=True)\n",
    "\n",
    "# Overall summary\n",
    "if cg_results and scd_results:\n",
    "    total_missing = cg_results['missing_count'] + scd_results['missing_count']\n",
    "    \n",
    "    pipeline.log_message(f\"OVERALL RECONCILIATION SUMMARY\")\n",
    "\n",
    "    pipeline.log_message(f\"CG Missing Files:   {cg_results['missing_count']:,}\")\n",
    "    pipeline.log_message(f\"SCD Missing Files:  {scd_results['missing_count']:,}\")\n",
    "    pipeline.log_message(f\"Total Missing:      {total_missing:,}\")\n",
    "    \n",
    "    if total_missing == 0:\n",
    "        pipeline.log_message(f\"All files processed successfully! No action required.\")\n",
    "    else:\n",
    "        pipeline.log_message(f\"\\nAction required: {total_missing} file(s) need processing\")\n",
    "    \n",
    "    pipeline.log_message(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec568a1-2c55-42e1-bb93-3c4fc3bb86d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Patronage Table Data Analysis"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATRONAGE TABLE DATA ANALYSIS - Quick Analytics Dashboard\n",
    "# =============================================================================\n",
    "\n",
    "# Import required modules\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"PATRONAGE TABLE DATA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the table name from the pipeline module\n",
    "table_name = pipeline.PATRONAGE_TABLE_NAME\n",
    "\n",
    "try:\n",
    "    print(f\"Analyzing table: {table_name}\")\n",
    "    print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 1. Total Active records for both CG and SCD\n",
    "    print(\"ACTIVE RECORDS BY BATCH TYPE\")\n",
    "    active_by_batch = spark.sql(f\"\"\"\n",
    "        SELECT Batch_CD, \n",
    "               COUNT(*) as Active_Records\n",
    "        FROM {table_name}\n",
    "        WHERE RecordStatus = true\n",
    "        GROUP BY Batch_CD\n",
    "        ORDER BY Batch_CD\n",
    "    \"\"\")\n",
    "    active_by_batch.display()\n",
    "    \n",
    "    # 2. Total Null ICN for CG\n",
    "    print(\"NULL ICN ANALYSIS FOR CG RECORDS\")\n",
    "    cg_null_icn = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as Total_CG_Active_Records,\n",
    "            SUM(CASE WHEN ICN IS NULL THEN 1 ELSE 0 END) as Null_ICN_Count,\n",
    "            SUM(CASE WHEN ICN IS NOT NULL THEN 1 ELSE 0 END) as Valid_ICN_Count,\n",
    "            ROUND((SUM(CASE WHEN ICN IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*)), 2) as Null_ICN_Percentage\n",
    "        FROM {table_name}\n",
    "        WHERE Batch_CD = 'CG' AND RecordStatus = true\n",
    "    \"\"\")\n",
    "    cg_null_icn.display()\n",
    "    \n",
    "    # 3. Total active records for PT_Indicator (both 'Y' and 'N') active records only\n",
    "    print(\"PT INDICATOR DISTRIBUTION (ACTIVE RECORDS ONLY)\")\n",
    "    pt_indicator_analysis = spark.sql(f\"\"\"\n",
    "        SELECT PT_Indicator,\n",
    "               COUNT(*) as Record_Count,\n",
    "               ROUND((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()), 2) as Percentage\n",
    "        FROM {table_name}\n",
    "        WHERE RecordStatus = true AND Batch_CD = 'SCD'\n",
    "        GROUP BY PT_Indicator\n",
    "        ORDER BY PT_Indicator\n",
    "    \"\"\")\n",
    "    pt_indicator_analysis.display()\n",
    "\n",
    "    # 4. Null participant_id analysis for SCD\n",
    "    print(\"NULL participant_id ANALYSIS FOR SCD RECORDS\")\n",
    "    scd_null_participant_id = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) as Total_SCD_Active_Records,\n",
    "            SUM(CASE WHEN participant_id IS NULL THEN 1 ELSE 0 END) as Null_participant_id_Count,\n",
    "            SUM(CASE WHEN participant_id IS NOT NULL THEN 1 ELSE 0 END) as Valid_participant_id_Count,\n",
    "            ROUND((SUM(CASE WHEN participant_id IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*)), 2) as Null_participant_id_Percentage\n",
    "        FROM {table_name}\n",
    "        WHERE Batch_CD = 'SCD' AND RecordStatus = true\n",
    "    \"\"\")\n",
    "    scd_null_participant_id.display()\n",
    "    \n",
    "    # 5. Active records per SDP_Event_Created_Timestamp \n",
    "    print(\"PROCESSING DATES BY RECORD COUNT\")\n",
    "    records_by_timestamp = spark.sql(f\"\"\"\n",
    "        SELECT DATE(SDP_Event_Created_Timestamp) as Processing_Date,\n",
    "               COUNT(*) as Record_Count\n",
    "        FROM {table_name}\n",
    "        WHERE RecordStatus = true\n",
    "        GROUP BY DATE(SDP_Event_Created_Timestamp)\n",
    "        ORDER BY Processing_Date DESC, Record_Count DESC\n",
    "    \"\"\")\n",
    "    records_by_timestamp.display()\n",
    "    \n",
    "    # 6. Active Record count for each Applicant_Type\n",
    "    print(\"ACTIVE RECORDS BY APPLICANT TYPE\")\n",
    "    records_by_applicant_type = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COALESCE(Applicant_Type, 'NULL/Not Specified') as Applicant_Type,\n",
    "            COUNT(*) as Record_Count,\n",
    "            ROUND((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()), 2) as Percentage\n",
    "        FROM {table_name}\n",
    "        WHERE RecordStatus = true AND Batch_CD = 'CG'\n",
    "        GROUP BY Applicant_Type\n",
    "        ORDER BY Record_Count DESC\n",
    "    \"\"\")\n",
    "    records_by_applicant_type.display()\n",
    "    \n",
    "    # 7. Active Record count for each Caregiver_Status\n",
    "    print(\"ACTIVE RECORDS BY CAREGIVER STATUS\")\n",
    "    records_by_caregiver_status = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COALESCE(Caregiver_Status, 'NULL/Not Specified') as Caregiver_Status,\n",
    "            COUNT(*) as Record_Count,\n",
    "            ROUND((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()), 2) as Percentage\n",
    "        FROM {table_name}\n",
    "        WHERE RecordStatus = true AND Batch_CD = 'CG'\n",
    "        GROUP BY Caregiver_Status\n",
    "        ORDER BY Record_Count DESC\n",
    "    \"\"\")\n",
    "    records_by_caregiver_status.display()\n",
    "    \n",
    "    # 8. Record count for each RecordStatus \n",
    "    print(\"RECORD STATUS DISTRIBUTION\")\n",
    "    records_by_status = spark.sql(f\"\"\"\n",
    "        SELECT RecordStatus,\n",
    "               COUNT(*) as Record_Count,\n",
    "               ROUND((COUNT(*) * 100.0 / SUM(COUNT(*)) OVER ()), 2) as Percentage\n",
    "        FROM {table_name}\n",
    "        GROUP BY RecordStatus\n",
    "        ORDER BY RecordStatus DESC\n",
    "    \"\"\")\n",
    "    records_by_status.display()\n",
    "    \n",
    "    # 9. Record counts per filename \n",
    "    print(\"FILES BY RECORD COUNT\")\n",
    "    records_by_filename = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN filename LIKE '%/%' THEN REVERSE(SPLIT(REVERSE(filename), '/')[0])\n",
    "                ELSE filename\n",
    "            END as File_Name,\n",
    "            COUNT(*) as Record_Count,\n",
    "            MIN(SDP_Event_Created_Timestamp) as Processing_Timestamp\n",
    "        FROM {table_name}\n",
    "        GROUP BY filename\n",
    "        ORDER BY Processing_Timestamp DESC\n",
    "    \"\"\")\n",
    "    records_by_filename.display(truncate=False)\n",
    "    \n",
    "    # 10. Overall Summary Statistics\n",
    "    print(\"\\nOVERALL SUMMARY STATISTICS\")\n",
    "    summary_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as Total_Records,\n",
    "            SUM(CASE WHEN RecordChangeStatus = 'New Record' THEN 1 ELSE 0 END) as New_Records,\n",
    "            SUM(CASE WHEN RecordChangeStatus = 'Updated Record' THEN 1 ELSE 0 END) as Updated_Records,\n",
    "            SUM(CASE WHEN RecordChangeStatus = 'Expired Record' THEN 1 ELSE 0 END) as Expired_Records,\n",
    "            COUNT(DISTINCT filename) as Total_Files_Processed,\n",
    "            COUNT(DISTINCT Batch_CD) as Batch_Types,\n",
    "            MIN(SDP_Event_Created_Timestamp) as Earliest_Record,\n",
    "            MAX(SDP_Event_Created_Timestamp) as Latest_Record\n",
    "        FROM {table_name}\n",
    "    \"\"\")\n",
    "    summary_stats.display(truncate=False)\n",
    "    \n",
    "    # 11. Batch Code Processing Timeline\n",
    "    print(\"BATCH CODE PROCESSING TIMELINE\")\n",
    "    batch_timeline_stats = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            Batch_CD as Batch_Type,\n",
    "            MIN(SDP_Event_Created_Timestamp) as Earliest_Record,\n",
    "            MAX(SDP_Event_Created_Timestamp) as Latest_Record,\n",
    "            COUNT(*) as Total_Records,\n",
    "            COUNT(DISTINCT filename) as Files_Processed\n",
    "        FROM {table_name}\n",
    "        GROUP BY Batch_CD\n",
    "        ORDER BY Batch_CD\n",
    "    \"\"\")\n",
    "    batch_timeline_stats.display()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"Data analysis completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during data analysis: {str(e)}\")\n",
    "    print(\"Please ensure the table exists and is accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486da910-b8e5-4557-b036-5077f8d677b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comparing data on latest TS for Staging and Patronage"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Get the table name from the pipeline module\n",
    "table_name = pipeline.PATRONAGE_TABLE_NAME\n",
    "\n",
    "def _get_latest_timestamp(df, batch_cd=None):\n",
    "    if batch_cd:\n",
    "        df = df.filter(F.col(\"Batch_CD\") == batch_cd)\n",
    "    ts_row = (\n",
    "        df.select(F.max(\"SDP_Event_Created_Timestamp\").alias(\"max_ts\"))\n",
    "          .filter(F.col(\"max_ts\").isNotNull())\n",
    "          .collect()\n",
    "    )\n",
    "    return ts_row[0][\"max_ts\"] if ts_row else None\n",
    "\n",
    "def compare_latest(batch_cd: str):\n",
    "    batch_cd = batch_cd.upper()\n",
    "    if batch_cd not in (\"SCD\", \"CG\"):\n",
    "        raise ValueError(\"batch_cd must be 'SCD' or 'CG'\")\n",
    "\n",
    "    patronage_base = spark.table(table_name).filter(\n",
    "        (F.col(\"RecordStatus\") == True) & (F.col(\"Batch_CD\") == batch_cd) & F.col(\"edipi\").isNotNull()\n",
    "    )\n",
    "    staging_path = (\n",
    "        \"delta.`/mnt/Patronage/SCD_Staging`\"\n",
    "        if batch_cd == \"SCD\"\n",
    "        else \"delta.`/mnt/Patronage/Caregivers_Staging_New`\"\n",
    "    )\n",
    "    staging_base = spark.sql(f\"SELECT * FROM {staging_path}\").filter(F.col(\"edipi\").isNotNull())\n",
    "\n",
    "    latest_ts = _get_latest_timestamp(patronage_base)\n",
    "    latest_stg_ts = _get_latest_timestamp(staging_base)\n",
    "\n",
    "    # align to the newer of the two so we compare the freshest data each table has\n",
    "    compare_ts = max(ts for ts in [latest_ts, latest_stg_ts] if ts is not None)\n",
    "    ts_lit = F.lit(compare_ts)\n",
    "\n",
    "    pat_df = patronage_base.filter(F.col(\"SDP_Event_Created_Timestamp\") == ts_lit).select(\"edipi\").dropDuplicates()\n",
    "    stg_df = staging_base.filter(F.col(\"SDP_Event_Created_Timestamp\") == ts_lit).select(\"edipi\").dropDuplicates()\n",
    "\n",
    "    in_pat_not_stg = pat_df.join(stg_df, \"edipi\", \"left_anti\")\n",
    "    in_stg_not_pat = stg_df.join(pat_df, \"edipi\", \"left_anti\")\n",
    "\n",
    "    counts = {\n",
    "        \"batch_cd\": batch_cd,\n",
    "        \"timestamp_compared\": compare_ts,\n",
    "        \"patronage_count\": pat_df.count(),\n",
    "        \"staging_count\": stg_df.count(),\n",
    "        \"in_patronage_only\": in_pat_not_stg.count(),\n",
    "        \"in_staging_only\": in_stg_not_pat.count(),\n",
    "    }\n",
    "\n",
    "    print(f\"\\nBatch {batch_cd} @ {compare_ts}\")\n",
    "    for k, v in counts.items():\n",
    "        if k not in (\"batch_cd\", \"timestamp_compared\"):\n",
    "            print(f\"{k}: {v:,}\")\n",
    "\n",
    "    return {\n",
    "        \"counts\": counts,\n",
    "        \"patronage_only\": in_pat_not_stg,\n",
    "        \"staging_only\": in_stg_not_pat,\n",
    "    }\n",
    "\n",
    "# Run for both batches\n",
    "result_scd = compare_latest(\"SCD\")\n",
    "result_cg  = compare_latest(\"CG\")\n",
    "\n",
    "# Optional: inspect EDIPIs\n",
    "result_scd[\"patronage_only\"].display()\n",
    "result_scd[\"staging_only\"].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6eed68f-025f-4cdd-8f8f-0eca86d1a6c0",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1766159252142}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Staging Dates\":201},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764695365281}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "Display last 10 files processed for Caregivers and SCD (Staging vs Patronage)"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from patronage_modularized import config as pipeline\n",
    "\n",
    "# --- Caregivers ---\n",
    "cg_staging_tbl = \"delta.`/mnt/Patronage/Caregivers_Staging_New`\"\n",
    "cg_patronage_tbl = pipeline.PATRONAGE_TABLE_NAME\n",
    "\n",
    "\n",
    "cg_staging_df = spark.sql(f\"\"\"\n",
    "SELECT 'Staging CG' as table_name, SDP_Event_Created_Timestamp as dates, COUNT(*) as count, \n",
    "       ROW_NUMBER() OVER (ORDER BY SDP_Event_Created_Timestamp DESC) as rn\n",
    "FROM {cg_staging_tbl}\n",
    "WHERE Batch_CD = 'CG'\n",
    "GROUP BY SDP_Event_Created_Timestamp\n",
    "ORDER BY SDP_Event_Created_Timestamp DESC\n",
    "\"\"\").filter(\"rn <= 10\")\n",
    "\n",
    "# Single DF for patronage\n",
    "cg_patronage_df = spark.sql(f\"\"\"\n",
    "SELECT 'Patronage CG' as table_name, SDP_Event_Created_Timestamp as dates, COUNT(*) as count, \n",
    "       ROW_NUMBER() OVER (ORDER BY SDP_Event_Created_Timestamp DESC) as rn\n",
    "FROM {cg_patronage_tbl}\n",
    "WHERE Batch_CD = 'CG'\n",
    "GROUP BY SDP_Event_Created_Timestamp\n",
    "ORDER BY SDP_Event_Created_Timestamp DESC\n",
    "\"\"\").filter(\"rn <= 10\")\n",
    "\n",
    "# Join side by side\n",
    "cg_combined_df = (\n",
    "    cg_staging_df.alias(\"s\")\n",
    "    .join(cg_patronage_df.alias(\"p\"), \"rn\", \"outer\")\n",
    "    .select(\n",
    "        F.col(\"s.table_name\").alias(\"Staging Table Name\"),\n",
    "        F.col(\"s.dates\").alias(\"Staging Dates\"),\n",
    "        F.col(\"s.count\").alias(\"Staging Count\"),\n",
    "        F.col(\"p.table_name\").alias(\"Patronage Table Name\"),\n",
    "        F.col(\"p.dates\").alias(\"Patronage Dates\"),\n",
    "        F.col(\"p.count\").alias(\"Patronage Count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline.log_message(\"Caregivers Comparison\")\n",
    "cg_combined_df.display()\n",
    "\n",
    "# --- SCD ---\n",
    "scd_staging_tbl = \"delta.`/mnt/Patronage/SCD_Staging`\"\n",
    "scd_patronage_tbl = pipeline.PATRONAGE_TABLE_NAME\n",
    "\n",
    "# Single DF for staging\n",
    "scd_staging_df = spark.sql(f\"\"\"\n",
    "SELECT 'Staging SCD' as table_name, SDP_Event_Created_Timestamp as dates, COUNT(*) as count, \n",
    "       ROW_NUMBER() OVER (ORDER BY SDP_Event_Created_Timestamp DESC) as rn\n",
    "FROM {scd_staging_tbl}\n",
    "WHERE Batch_CD = 'SCD'\n",
    "GROUP BY SDP_Event_Created_Timestamp\n",
    "ORDER BY SDP_Event_Created_Timestamp DESC\n",
    "\"\"\").filter(\"rn <= 10\")\n",
    "\n",
    "# Single DF for patronage\n",
    "scd_patronage_df = spark.sql(f\"\"\"\n",
    "SELECT 'Patronage SCD' as table_name, SDP_Event_Created_Timestamp as dates, COUNT(*) as count, \n",
    "       ROW_NUMBER() OVER (ORDER BY SDP_Event_Created_Timestamp DESC) as rn\n",
    "FROM {scd_patronage_tbl}\n",
    "WHERE Batch_CD = 'SCD'\n",
    "GROUP BY SDP_Event_Created_Timestamp\n",
    "ORDER BY SDP_Event_Created_Timestamp DESC\n",
    "\"\"\").filter(\"rn <= 10\")\n",
    "\n",
    "# Join side by side\n",
    "scd_combined_df = (\n",
    "    scd_staging_df.alias(\"s\")\n",
    "    .join(scd_patronage_df.alias(\"p\"), \"rn\", \"outer\")\n",
    "    .select(\n",
    "        F.col(\"s.table_name\").alias(\"Staging Table Name\"),\n",
    "        F.col(\"s.dates\").alias(\"Staging Dates\"),\n",
    "        F.col(\"s.count\").alias(\"Staging Count\"),\n",
    "        F.col(\"p.table_name\").alias(\"Patronage Table Name\"),\n",
    "        F.col(\"p.dates\").alias(\"Patronage Dates\"),\n",
    "        F.col(\"p.count\").alias(\"Patronage Count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline.log_message(\"SCD Comparison\")\n",
    "scd_combined_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7ac194e7-32a3-4a4c-922f-87c4a3f278ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unit Testing with Dummy Data"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, DateType\n",
    "# from datetime import datetime\n",
    "\n",
    "# import patronage_modularized.transforms as transforms\n",
    "# import patronage_modularized.state as state\n",
    "# from patronage_modularized import config as pipeline\n",
    "\n",
    "# # 1. Define Schemas\n",
    "# # -----------------\n",
    "# scd_schema = StructType([\n",
    "#     StructField(\"PTCPNT_ID\", StringType(), True),\n",
    "#     StructField(\"CMBNED_DEGREE_DSBLTY\", StringType(), True),\n",
    "#     StructField(\"DSBL_DTR_DT\", StringType(), True),\n",
    "#     StructField(\"_metadata\", StructType([\n",
    "#         StructField(\"file_path\", StringType(), True),\n",
    "#         StructField(\"file_modification_time\", TimestampType(), True)\n",
    "#     ]), True)\n",
    "# ])\n",
    "\n",
    "# target_schema = StructType([\n",
    "#     StructField(\"participant_id\", StringType(), True),\n",
    "#     StructField(\"SC_Combined_Disability_Percentage\", StringType(), True),\n",
    "#     StructField(\"PT_Indicator\", StringType(), True),\n",
    "#     StructField(\"Status_Begin_Date\", StringType(), True),\n",
    "#     StructField(\"Status_Last_Update\", StringType(), True),\n",
    "#     StructField(\"RecordStatus\", BooleanType(), True),\n",
    "#     StructField(\"Batch_CD\", StringType(), True),\n",
    "#     StructField(\"ICN\", StringType(), True) # Needed for identity join\n",
    "# ])\n",
    "\n",
    "# pt_schema = StructType([\n",
    "#     StructField(\"participant_id\", StringType(), True),\n",
    "#     StructField(\"pt_indicator_value\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# identity_schema = StructType([\n",
    "#     StructField(\"participant_id\", StringType(), True),\n",
    "#     StructField(\"ICN\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # 2. Create Dummy Data\n",
    "# # --------------------\n",
    "\n",
    "# # A. Source Data (Daily File)\n",
    "# # - P001: Update SC % (50 -> 70). (Will also get PT update from PT Data) -> Scenario: Both Update\n",
    "# # - P005: New Record (SC=20). (Not in PT Data) -> Scenario: New Record SC Only\n",
    "# # - P005: Duplicate Record (to test dedupe)\n",
    "# # - P006: Update SC % (30 -> 40). (Not in PT Data) -> Scenario: Update Disability% Only\n",
    "# # - P007: No SC Change (10 -> 10). (Will get PT update from PT Data) -> Scenario: Update PT_Indicator Only (Direct)\n",
    "# source_data = [\n",
    "#     (\"P001\", \"70\", \"11012025\", (\"dummy_file.csv\", datetime.now())),\n",
    "#     (\"P005\", \"20\", \"11012025\", (\"dummy_file.csv\", datetime.now())),\n",
    "#     (\"P005\", \"20\", \"11012025\", (\"dummy_file.csv\", datetime.now())),\n",
    "#     (\"P006\", \"40\", \"11012025\", (\"dummy_file.csv\", datetime.now())),\n",
    "#     (\"P007\", \"10\", \"11012025\", (\"dummy_file.csv\", datetime.now()))\n",
    "# ]\n",
    "# dummy_source_df = spark.createDataFrame(source_data, scd_schema)\n",
    "\n",
    "# # B. Target Data (Existing Delta Table)\n",
    "# # - P001: Active, SC=050, PT=N (Will be updated by Source SC + PT Data)\n",
    "# # - P002: Active, SC=030, PT=N (Will be backfilled by PT Data) -> Scenario: Update (Backfill) PT_Indicator\n",
    "# # - P003: Active, SC=010, PT=N (No Change)\n",
    "# # - P004: Active, SC=100, PT=Y (No Change)\n",
    "# # - P006: Active, SC=030, PT=N (Will be updated by Source SC only)\n",
    "# # - P007: Active, SC=010, PT=N (Will be updated by PT Data via Direct path)\n",
    "# target_data = [\n",
    "#     (\"P001\", \"050\", \"N\", \"20240101\", \"20240101\", True, \"SCD\", \"ICN001\"),\n",
    "\n",
    "# target_data = [\n",
    "#     (\"P001\", \"050\", \"N\", \"20240101\", \"20240101\", True, \"SCD\", \"ICN001\"),\n",
    "#     (\"P002\", \"030\", \"N\", \"20240101\", \"20240101\", True, \"SCD\", \"ICN002\"),\n",
    "#     (\"P003\", \"010\", \"N\", \"20240101\", \"20240101\", True, \"SCD\", \"ICN003\"),\n",
    "# dummy_target_df = spark.createDataFrame(target_data, target_schema)\n",
    "\n",
    "# # C. PT Data (Reference Table)\n",
    "# # - P001: Y (Triggers PT update for P001, combined with Source SC update)\n",
    "# # - P002: Y (Triggers Backfill for P002)\n",
    "# # - P004: Y (Matches existing P004)\n",
    "# # - P007: Y (Triggers PT update for P007 via Direct path)\n",
    "# pt_data = [\n",
    "#     (\"P001\", \"Y\"),\n",
    "#     (\"P002\", \"Y\"),\n",
    "#     (\"P004\", \"Y\"),\n",
    "#     (\"P007\", \"Y\")\n",
    "# ]\n",
    "# dummy_pt_df = spark.createDataFrame(pt_data, pt_schema)\n",
    "\n",
    "# # D. Identity Data (Mocked identity_lookup_table)\n",
    "# # Maps participant_id to ICN\n",
    "# identity_data = [\n",
    "#     (\"P001\", \"ICN001\"),\n",
    "# dummy_pt_df = spark.createDataFrame(pt_data, pt_schema)\n",
    "\n",
    "# # D. Identity Data (Mocked identity_lookup_table)\n",
    "\n",
    "# ]\n",
    "# dummy_identity_df = spark.createDataFrame(identity_data, identity_schema)\n",
    "\n",
    "# print(\"Dummy Data Created:\")\n",
    "# print(f\"Source Records:   {dummy_source_df.count()} (Includes 1 duplicate)\")\n",
    "# print(f\"Target Records:   {dummy_target_df.count()}\")\n",
    "#     (\"P006\", \"ICN006\"),\n",
    "#     (\"P007\", \"ICN007\")\n",
    "# ]\n",
    "# # Override the pipeline's table name variable to point to our dummy view\n",
    "# original_table_name = pipeline.PATRONAGE_TABLE_NAME\n",
    "# pipeline.PATRONAGE_TABLE_NAME = dummy_target_view\n",
    "# transforms.PATRONAGE_TABLE_NAME = dummy_target_view\n",
    "\n",
    "# # Inject the dummy identity dataframe into the pipeline module\n",
    "# # This fixes the AttributeError: 'NoneType' object has no attribute '_jdf'\n",
    "# # The global variable in the module is 'identity_lookup_table', not 'filtered_identity'\n",
    "# state.identity_lookup_table = dummy_identity_df\n",
    "\n",
    "# print(f\"Environment Mocked. Pipeline now points to '{dummy_target_view}' instead of '{original_table_name}'\")\n",
    "# print(\"Injected dummy_identity_df into state.identity_lookup_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "102ef34d-2aac-43fd-b802-7db8bded595c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute Test Transformation Logic"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# # 4. Execute Transformation Logic\n",
    "# # -------------------------------\n",
    "# print(\"Running transform_scd_data with dummy inputs...\")\n",
    "\n",
    "# # We pass 'update' mode to trigger the standard logic\n",
    "# # Note: transform_scd_data returns the DataFrame of *changes* (records to be upserted/expired)\n",
    "# result_df = transforms.transform_scd_data(dummy_source_df, dummy_pt_df, 'update')\n",
    "\n",
    "# # Cache result for analysis\n",
    "# result_df.cache()\n",
    "# result_count = result_df.count()\n",
    "\n",
    "# print(f\"Transformation Complete. Found {result_count} records to process.\")\n",
    "# result_df.select(\"participant_id\", \"SC_Combined_Disability_Percentage\", \"target_SC_Combined_Disability_Percentage\", \"PT_Indicator\", \"target_PT_Indicator\", \"target_RecordStatus\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ef8f4082-52ac-4317-97cc-6dc8f2e134bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assertions and Validation for Test"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# # 5. Assertions and Validation\n",
    "# # ----------------------------\n",
    "# print(\"Validating Results...\")\n",
    "\n",
    "# # Collect results to a list of rows for easy checking\n",
    "# results = {row['participant_id']: row for row in result_df.collect()}\n",
    "\n",
    "# # Assertion 1: Count Check\n",
    "# # Expected: 5 records\n",
    "# # - P001 (Update SC + PT)\n",
    "# # - P005 (New - Deduplicated)\n",
    "# # - P002 (Backfill PT)\n",
    "# # - P006 (Update SC Only)\n",
    "# # - P007 (Update PT Only - Direct)\n",
    "# assert result_count == 5, f\"Expected 5 records, found {result_count}\"\n",
    "# print(\"Assertion Passed: Record count is 5.\")\n",
    "\n",
    "# # Assertion 2: Deduplication Check\n",
    "# # P005 appeared twice in source, should appear once in result\n",
    "# assert \"P005\" in results, \"P005 missing from results\"\n",
    "# # Check if P005 is marked as New (target_RecordStatus should be Null)\n",
    "# assert results[\"P005\"][\"target_RecordStatus\"] is None, \"P005 should be identified as a New Record\"\n",
    "# print(\"Assertion Passed: Deduplication worked (P005 present once).\")\n",
    "\n",
    "# # Assertion 3: Update Logic Check (Scenario: Update Both SC + PT)\n",
    "# # P001 should have SC=070 (Source) and PT=Y (PT Data)\n",
    "# p001 = results[\"P001\"]\n",
    "# assert p001[\"SC_Combined_Disability_Percentage\"] == \"070\", f\"P001 SC% mismatch. Expected 070, got {p001['SC_Combined_Disability_Percentage']}\"\n",
    "# assert p001[\"PT_Indicator\"] == \"Y\", f\"P001 PT mismatch. Expected Y, got {p001['PT_Indicator']}\"\n",
    "# print(\"Assertion Passed: Simultaneous Update (SC + PT) worked for P001.\")\n",
    "\n",
    "# # Assertion 4: Backfill Logic Check (Scenario: Update Backfill PT)\n",
    "# # P002 should have SC=030 (Target) and PT=Y (PT Data)\n",
    "# p002 = results[\"P002\"]\n",
    "# assert p002[\"SC_Combined_Disability_Percentage\"] == \"030\", f\"P002 SC% mismatch. Expected 030, got {p002['SC_Combined_Disability_Percentage']}\"\n",
    "# assert p002[\"PT_Indicator\"] == \"Y\", f\"P002 PT mismatch. Expected Y, got {p002['PT_Indicator']}\"\n",
    "# print(\"Assertion Passed: Backfill logic worked for P002.\")\n",
    "\n",
    "# # Assertion 5: No False Positives\n",
    "# # P003 (No change) and P004 (No change) should NOT be in results\n",
    "# assert \"P003\" not in results, \"P003 should not be in results (No change)\"\n",
    "# assert \"P004\" not in results, \"P004 should not be in results (No change)\"\n",
    "# print(\"Assertion Passed: Unchanged records correctly ignored.\")\n",
    "\n",
    "# # Assertion 6: New Record Logic Check (Scenario: Insert New Record)\n",
    "# # P005 should have SC=020 (Source) and PT=N (Default)\n",
    "# p005 = results[\"P005\"]\n",
    "# assert p005[\"SC_Combined_Disability_Percentage\"] == \"020\", f\"P005 SC% mismatch. Expected 020, got {p005['SC_Combined_Disability_Percentage']}\"\n",
    "# assert p005[\"PT_Indicator\"] == \"N\", f\"P005 PT mismatch. Expected N, got {p005['PT_Indicator']}\"\n",
    "# print(\"Assertion Passed: New Record (SC only) worked for P005.\")\n",
    "\n",
    "# # Assertion 7: Update SC Only Logic Check (Scenario: Update Disability% Only)\n",
    "# # P006 should have SC=040 (Source) and PT=N (Target/Default)\n",
    "# p006 = results[\"P006\"]\n",
    "# assert p006[\"SC_Combined_Disability_Percentage\"] == \"040\", f\"P006 SC% mismatch. Expected 040, got {p006['SC_Combined_Disability_Percentage']}\"\n",
    "# assert p006[\"PT_Indicator\"] == \"N\", f\"P006 PT mismatch. Expected N, got {p006['PT_Indicator']}\"\n",
    "# print(\"Assertion Passed: Update SC Only worked for P006.\")\n",
    "\n",
    "# # Assertion 8: Update PT Only (Direct) Logic Check (Scenario: Update PT_Indicator Only)\n",
    "# # P007 should have SC=010 (Source/Target) and PT=Y (PT Data)\n",
    "# p007 = results[\"P007\"]\n",
    "# assert p007[\"SC_Combined_Disability_Percentage\"] == \"010\", f\"P007 SC% mismatch. Expected 010, got {p007['SC_Combined_Disability_Percentage']}\"\n",
    "# assert p007[\"PT_Indicator\"] == \"Y\", f\"P007 PT mismatch. Expected Y, got {p007['PT_Indicator']}\"\n",
    "# print(\"Assertion Passed: Update PT Only (Direct) worked for P007.\")\n",
    "\n",
    "# # Cleanup\n",
    "# # Restore original table name\n",
    "# pipeline.PATRONAGE_TABLE_NAME = original_table_name\n",
    "# print(\"\\nTest Complete. Environment restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "87e202ba-e610-4a4a-894c-05d0d81622eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DMDC FILE COMPARISON"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# from patronage_modularized import config as pipeline\n",
    "# pipeline.log_message(\"\\n\" + \"=\" * 70)\n",
    "# pipeline.log_message(\"DMDC FILE COMPARISON\")\n",
    "# pipeline.log_message(\"=\" * 70)\n",
    "\n",
    "# file_name = 'PATRONAGE_20251226.txt' # 'BACKFILL_EDIPI_PATRONAGE_20251226.txt'\n",
    "# # File paths\n",
    "# staging_path = f\"/mnt/ci-patronage/dmdc_extracts/test/combined_export/{file_name}\"\n",
    "# patronage_path = f\"/mnt/ci-patronage/dmdc_extracts/combined_export/{file_name}\" # LAGGED_EDIPI_PATRONAGE_20251226.txt \n",
    "\n",
    "# pipeline.log_message(f\"\\nPatronage: {patronage_path}\")\n",
    "# pipeline.log_message(f\"Staging: {staging_path}\")\n",
    "\n",
    "# try:\n",
    "#     # Read both files as text\n",
    "#     patronage_raw = spark.read.text(patronage_path)\n",
    "#     staging_raw = spark.read.text(staging_path)\n",
    "    \n",
    "#     # Parse fixed-width format (42 chars total)\n",
    "#     # EDIPI (chars 1-10) | Batch_CD (11-13) | Disability % (14-16) | \n",
    "#     # Status_Begin_Date (17-24) | PT (25) | Unemployability (26) | \n",
    "#     # Status_Last_Update (27-34) | Status_Termination_Date (35-42)\n",
    "    \n",
    "#     query_columns = (\n",
    "#         F.trim(F.substring(F.col(\"value\"), 1, 10)).alias(\"edipi\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 11, 3)).alias(\"batch_cd\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 14, 3)).alias(\"disability_pct\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 17, 8)).alias(\"status_begin_date\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 25, 1)).alias(\"pt_indicator\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 26, 1)).alias(\"unemployability\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 27, 8)).alias(\"status_last_update\"),\n",
    "#         F.trim(F.substring(F.col(\"value\"), 35, 8)).alias(\"status_termination_date\"),\n",
    "#         F.col(\"value\").alias(\"full_record\")\n",
    "#     )\n",
    "\n",
    "#     filter_condition = (F.col(\"edipi\").isNotNull() & (F.col(\"edipi\") != \"\"))\n",
    "#     patronage = (\n",
    "#         patronage_raw\n",
    "#         .select(*query_columns)\n",
    "#         .filter(filter_condition)\n",
    "#     )\n",
    "    \n",
    "#     staging = (\n",
    "#         staging_raw\n",
    "#         .select(*query_columns)\n",
    "#         .filter(filter_condition)\n",
    "#     )\n",
    "    \n",
    "#     patronage_count = patronage.count()\n",
    "#     staging_count = staging.count()\n",
    "    \n",
    "#     pipeline.log_message(f\"\\nFile Counts:\")\n",
    "#     pipeline.log_message(f\"  Patronage: {patronage_count:,} records\")\n",
    "#     pipeline.log_message(f\"  Staging: {staging_count:,} records\")\n",
    "#     pipeline.log_message(f\"  Difference: {abs(patronage_count - staging_count):,} records\")\n",
    "    \n",
    "#     # Compare by EDIPI\n",
    "#     # EDIPIs in patronage but not in staging\n",
    "#     only_in_patronage = patronage.join(staging, \"edipi\", \"left_anti\")\n",
    "    \n",
    "#     # EDIPIs in staging but not in patronage\n",
    "#     only_in_staging = staging.join(patronage, \"edipi\", \"left_anti\")\n",
    "    \n",
    "#     count_only_patronage = only_in_patronage.count()\n",
    "#     count_only_staging = only_in_staging.count()\n",
    "    \n",
    "#     pipeline.log_message(f\"\\nEDIPI Comparison:\")\n",
    "#     pipeline.log_message(f\"  Only in Patronage: {count_only_patronage:,} EDIPIs\")\n",
    "#     pipeline.log_message(f\"  Only in Staging: {count_only_staging:,} EDIPIs\")\n",
    "    \n",
    "#     if count_only_patronage > 0:\n",
    "#         pipeline.log_message(f\"\\n--- {count_only_patronage:,} EDIPIs ONLY in Patronage ---\")\n",
    "#         only_in_patronage.select(\"*\").display()\n",
    "    \n",
    "#     if count_only_staging > 0:\n",
    "#         pipeline.log_message(f\"\\n--- {count_only_staging:,} EDIPIs ONLY in Staging ---\")\n",
    "#         only_in_staging.select(\"*\").display()\n",
    "    \n",
    "#     if count_only_patronage == 0 and count_only_staging == 0:\n",
    "#         pipeline.log_message(f\"\\nPerfect match! Both files have identical EDIPIs.\")\n",
    "#     else:\n",
    "#         pipeline.log_message(f\"\\nMismatch: {count_only_patronage + count_only_staging:,} EDIPIs differ\")\n",
    "    \n",
    "#     pipeline.log_message(\"=\" * 70)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     pipeline.log_message(f\"ERROR: {str(e)}\", level='ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6e17c1-aaac-43ea-b33d-9fc0f9cf26c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trace Participant IDs from Source to Destination\n",
    "\n",
    "Use `trace_participant_id(...)` to locate a participant ID across: raw SCD file → identity lookup → duplicates → SCD staging → `patronage_unified`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e76ae4f6-6d81-4912-8f1e-d72d693095f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Trace Participant ID's from Source to Destination"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# def trace_participant_id(\n",
    "#     file_name: str,\n",
    "#     participant_ids,\n",
    "#     *,\n",
    "#     sep: str = \",\",\n",
    "#     base_dir: str = \"dbfs:/mnt/ci-vadir-shared\",\n",
    "#     scd_staging_delta_path: str = \"/mnt/Patronage/SCD_Staging\",\n",
    "# ) -> None:\n",
    "#     ids = [str(x) for x in participant_ids]\n",
    "#     raw_path = file_name if file_name.startswith(\"dbfs:\") else f\"{base_dir.rstrip('/')}/{file_name.lstrip('/')}\"\n",
    "\n",
    "#     raw_df = (\n",
    "#         spark.read.option(\"header\", \"true\")\n",
    "#         .option(\"sep\", sep)\n",
    "#         .option(\"inferSchema\", \"false\")\n",
    "#         .csv(raw_path)\n",
    "#     )\n",
    "#     raw_hits = raw_df.filter(col(\"PTCPNT_ID\").cast(\"string\").isin(ids))\n",
    "#     print(f\"Raw SCD file: {raw_path}\")\n",
    "#     print(f\"Raw matches (PTCPNT_ID in {ids}): {raw_hits.count()}\")\n",
    "#     display(raw_hits)\n",
    "\n",
    "#     identity_df = spark.read.format(\"delta\").load(pipeline.IDENTITY_TABLE_PATH)\n",
    "#     identity_hits = identity_df.filter(col(\"participant_id\").cast(\"string\").isin(ids))\n",
    "#     print(f\"\\nIdentity lookup matches (participant_id in {ids}): {identity_hits.count()}\")\n",
    "#     display(identity_hits)\n",
    "\n",
    "#     dup_df = spark.read.format(\"delta\").load(pipeline.DUP_IDENTITY_TABLE_PATH)\n",
    "#     dup_hits = dup_df.filter(col(\"TreatingFacilityPersonIdentifier\").cast(\"string\").isin(ids))\n",
    "#     print(f\"\\nDuplicate correlations matches (TreatingFacilityPersonIdentifier in {ids}): {dup_hits.count()}\")\n",
    "#     display(dup_hits)\n",
    "\n",
    "#     staging_df = spark.read.format(\"delta\").load(scd_staging_delta_path)\n",
    "#     staging_hits = staging_df.filter(col(\"participant_id\").cast(\"string\").isin(ids))\n",
    "#     print(f\"\\nSCD staging matches (participant_id in {ids}): {staging_hits.count()}\")\n",
    "#     display(staging_hits)\n",
    "\n",
    "#     patronage_df = spark.table(\"patronage_unified\")\n",
    "#     patronage_hits = patronage_df.filter(col(\"participant_id\").cast(\"string\").isin(ids))\n",
    "#     print(f\"\\nPatronage unified matches (participant_id in {ids}): {patronage_hits.count()}\")\n",
    "#     display(patronage_hits)\n",
    "\n",
    "# # Usage\n",
    "# trace_participant_id(\"CPIDODIEX_20251227_spool.csv\", [\"67186394\", \"4744852\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5864390e-4238-4af7-a29a-f5e78c6158da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# WITH staging AS (\n",
    "#     SELECT \n",
    "#         edipi, participant_id, PT_Indicator, SC_Combined_Disability_Percentage\n",
    "#     FROM delta.`/mnt/Patronage/SCD_Staging`\n",
    "#     WHERE SDP_Event_Created_Timestamp = '2025-12-27T09:31:00.000+00:00'\n",
    "#         AND Batch_CD = 'SCD'\n",
    "# ),\n",
    "# patronage AS (\n",
    "#     SELECT \n",
    "#         edipi, participant_id, PT_Indicator, SC_Combined_Disability_Percentage\n",
    "#     FROM patronage_unified\n",
    "#     WHERE SDP_Event_Created_Timestamp = '2025-12-27T09:31:00.000+00:00'\n",
    "#         AND Batch_CD = 'SCD'\n",
    "#         AND RecordStatus = TRUE\n",
    "# )\n",
    "# SELECT \n",
    "#     COALESCE(s.participant_id, p.participant_id) as participant_id,\n",
    "#     s.PT_Indicator as staging_PT,\n",
    "#     p.PT_Indicator as patronage_PT,\n",
    "#     CASE WHEN s.PT_Indicator != p.PT_Indicator THEN 'MISMATCH' ELSE 'MATCH' END as PT_Status,\n",
    "#     s.SC_Combined_Disability_Percentage as staging_Disability_Pct,\n",
    "#     p.SC_Combined_Disability_Percentage as patronage_Disability_Pct,\n",
    "#     CASE WHEN s.SC_Combined_Disability_Percentage != p.SC_Combined_Disability_Percentage THEN 'MISMATCH' ELSE 'MATCH' END as Disability_Status\n",
    "# FROM staging s\n",
    "# FULL OUTER JOIN patronage p\n",
    "# WHERE s.PT_Indicator != p.PT_Indicator \n",
    "#    OR s.SC_Combined_Disability_Percentage != p.SC_Combined_Disability_Percentage\n",
    "# ORDER BY COALESCE(s.edipi, p.edipi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb3913c4-abd4-4bb4-a802-62fda95a9452",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"lastModified\":219},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766503608575}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = spark.sql(\"DESCRIBE DETAIL delta.`/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/`\")\n",
    "# display(df.select(df.lastModified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Log Dashboard — Core Queries\n",
    "\n",
    "Use the queries below to power a 360° operational dashboard from the pipeline log table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE LOG DASHBOARD — CORE QUERIES\n",
    "# =============================================================================\n",
    "# Dashboard-ready queries for Patronage pipeline logging\n",
    "log_table = pipeline.PATRONAGE_PIPELINE_LOG_TABLE_NAME\n",
    "\n",
    "# 1) Latest runs overview\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        processing_mode,\n",
    "        status,\n",
    "        duration_seconds\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 30\n",
    "\"\"\").display()\n",
    "\n",
    "# 2) Recent failures (for backtracing)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        processing_mode,\n",
    "        status,\n",
    "        error_message\n",
    "    FROM {log_table}\n",
    "    WHERE status = 'FAILED'\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 30\n",
    "\"\"\").display()\n",
    "\n",
    "# 3) Input watermarks + file discovery summary\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(input_watermarks, '$.SCD_last_processed_ts') AS scd_last_processed_ts,\n",
    "        get_json_object(input_watermarks, '$.CG_last_processed_ts') AS cg_last_processed_ts,\n",
    "        get_json_object(file_discovery_detail, '$.total_scd_files') AS total_scd_files,\n",
    "        get_json_object(file_discovery_detail, '$.total_cg_files') AS total_cg_files\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 30\n",
    "\"\"\").display()\n",
    "\n",
    "# 4) DMDC + EDIPI backfill + backup status\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(dmdc_export_stats, '$.record_count') AS dmdc_record_count,\n",
    "        get_json_object(dmdc_export_stats, '$.filename') AS dmdc_filename,\n",
    "        get_json_object(edipi_backfill_stats, '$.record_count') AS edipi_record_count,\n",
    "        get_json_object(edipi_backfill_stats, '$.filename') AS edipi_filename,\n",
    "        get_json_object(backup_stats, '$.status') AS backup_status\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 30\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Log Dashboard — Extended 360° Views\n",
    "\n",
    "Run any section below to populate dashboard visuals and trend monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE LOG DASHBOARD — EXTENDED 360° VIEWS\n",
    "# =============================================================================\n",
    "# Extended dashboard queries for Patronage pipeline logging\n",
    "log_table = pipeline.PATRONAGE_PIPELINE_LOG_TABLE_NAME\n",
    "\n",
    "# 5) Run volume + success rate (last 90 days)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        DATE(run_timestamp_utc) AS run_date,\n",
    "        COUNT(*) AS total_runs,\n",
    "        SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) AS success_runs,\n",
    "        SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) AS failed_runs\n",
    "    FROM {log_table}\n",
    "    WHERE run_timestamp_utc >= date_sub(current_date(), 90)\n",
    "    GROUP BY DATE(run_timestamp_utc)\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()\n",
    "\n",
    "# 6) Processing mode split + duration trend\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        processing_mode,\n",
    "        COUNT(*) AS runs,\n",
    "        ROUND(AVG(duration_seconds), 2) AS avg_duration_sec,\n",
    "        ROUND(MAX(duration_seconds), 2) AS max_duration_sec\n",
    "    FROM {log_table}\n",
    "    GROUP BY processing_mode\n",
    "    ORDER BY runs DESC\n",
    "\"\"\").display()\n",
    "\n",
    "# 7) Raw vs processed rows (SCD/CG) over time\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(records_processed_detail, '$.raw_scd_rows') AS raw_scd_rows,\n",
    "        get_json_object(records_processed_detail, '$.processed_scd_rows') AS processed_scd_rows,\n",
    "        get_json_object(records_processed_detail, '$.raw_cg_rows') AS raw_cg_rows,\n",
    "        get_json_object(records_processed_detail, '$.processed_cg_rows') AS processed_cg_rows\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 100\n",
    "\"\"\").display()\n",
    "\n",
    "# 8) Active/New/Expired output counts by batch\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(output_row_counts_by_batch, '$.SCD.active') AS scd_active,\n",
    "        get_json_object(output_row_counts_by_batch, '$.SCD.new') AS scd_new,\n",
    "        get_json_object(output_row_counts_by_batch, '$.SCD.expired') AS scd_expired,\n",
    "        get_json_object(output_row_counts_by_batch, '$.CG.active') AS cg_active,\n",
    "        get_json_object(output_row_counts_by_batch, '$.CG.new') AS cg_new,\n",
    "        get_json_object(output_row_counts_by_batch, '$.CG.expired') AS cg_expired\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 100\n",
    "\"\"\").display()\n",
    "\n",
    "# 9) Scheduling events overview (DMDC, EDIPI, Backups)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(scheduled_tasks_detail, '$.dmdc_export_triggered') AS dmdc_triggered,\n",
    "        get_json_object(scheduled_tasks_detail, '$.edipi_backfill_triggered') AS edipi_triggered,\n",
    "        get_json_object(scheduled_tasks_detail, '$.backup_triggered') AS backup_triggered,\n",
    "        get_json_object(dmdc_export_stats, '$.record_count') AS dmdc_records,\n",
    "        get_json_object(edipi_backfill_stats, '$.record_count') AS edipi_records,\n",
    "        get_json_object(backup_stats, '$.status') AS backup_status\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 100\n",
    "\"\"\").display()\n",
    "\n",
    "# 10) File discovery volumes vs watermarks\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(input_watermarks, '$.SCD_last_processed_ts') AS scd_watermark,\n",
    "        get_json_object(input_watermarks, '$.CG_last_processed_ts') AS cg_watermark,\n",
    "        get_json_object(file_discovery_detail, '$.total_scd_files') AS scd_files,\n",
    "        get_json_object(file_discovery_detail, '$.total_cg_files') AS cg_files\n",
    "    FROM {log_table}\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 100\n",
    "\"\"\").display()\n",
    "\n",
    "# 11) Runtime outliers (top 20 longest runs)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        processing_mode,\n",
    "        status,\n",
    "        duration_seconds,\n",
    "        error_message\n",
    "    FROM {log_table}\n",
    "    ORDER BY duration_seconds DESC\n",
    "    LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Log Dashboard — Daily Aggregates & Operational Health\n",
    "\n",
    "These queries provide daily rollups and last-known statuses for exports, backfills, and backups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE LOG DASHBOARD — DAILY AGGREGATES & HEALTH\n",
    "# =============================================================================\n",
    "# Daily aggregates and operational health queries\n",
    "log_table = pipeline.PATRONAGE_PIPELINE_LOG_TABLE_NAME\n",
    "\n",
    "# A) Daily run health (success/fail + avg duration)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        DATE(run_timestamp_utc) AS run_date,\n",
    "        COUNT(*) AS total_runs,\n",
    "        SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) AS success_runs,\n",
    "        SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) AS failed_runs,\n",
    "        ROUND(AVG(duration_seconds), 2) AS avg_duration_sec\n",
    "    FROM {log_table}\n",
    "    GROUP BY DATE(run_timestamp_utc)\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()\n",
    "\n",
    "# B) Daily raw vs processed row totals\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        DATE(run_timestamp_utc) AS run_date,\n",
    "        SUM(CAST(get_json_object(records_processed_detail, '$.raw_scd_rows') AS BIGINT)) AS raw_scd_rows,\n",
    "        SUM(CAST(get_json_object(records_processed_detail, '$.processed_scd_rows') AS BIGINT)) AS processed_scd_rows,\n",
    "        SUM(CAST(get_json_object(records_processed_detail, '$.raw_cg_rows') AS BIGINT)) AS raw_cg_rows,\n",
    "        SUM(CAST(get_json_object(records_processed_detail, '$.processed_cg_rows') AS BIGINT)) AS processed_cg_rows\n",
    "    FROM {log_table}\n",
    "    GROUP BY DATE(run_timestamp_utc)\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()\n",
    "\n",
    "# C) Latest successful pipeline run (timestamp + mode)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        processing_mode,\n",
    "        duration_seconds\n",
    "    FROM {log_table}\n",
    "    WHERE status = 'SUCCESS'\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 1\n",
    "\"\"\").display()\n",
    "\n",
    "# D) Latest DMDC export run (if triggered)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(dmdc_export_stats, '$.record_count') AS record_count,\n",
    "        get_json_object(dmdc_export_stats, '$.filename') AS filename\n",
    "    FROM {log_table}\n",
    "    WHERE get_json_object(dmdc_export_stats, '$.triggered') = 'true'\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 1\n",
    "\"\"\").display()\n",
    "\n",
    "# E) Latest EDIPI backfill run (if triggered)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(edipi_backfill_stats, '$.record_count') AS record_count,\n",
    "        get_json_object(edipi_backfill_stats, '$.filename') AS filename\n",
    "    FROM {log_table}\n",
    "    WHERE get_json_object(edipi_backfill_stats, '$.triggered') = 'true'\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 1\n",
    "\"\"\").display()\n",
    "\n",
    "# F) Latest monthly backup run (if triggered)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(backup_stats, '$.status') AS backup_status\n",
    "    FROM {log_table}\n",
    "    WHERE get_json_object(backup_stats, '$.triggered') = 'true'\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 1\n",
    "\"\"\").display()\n",
    "\n",
    "# G) Latest run with unprocessed files (potential backlog)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        run_timestamp_utc,\n",
    "        get_json_object(file_discovery_detail, '$.total_scd_files') AS total_scd_files,\n",
    "        get_json_object(file_discovery_detail, '$.total_cg_files') AS total_cg_files\n",
    "    FROM {log_table}\n",
    "    WHERE CAST(get_json_object(file_discovery_detail, '$.total_scd_files') AS INT) > 0\n",
    "       OR CAST(get_json_object(file_discovery_detail, '$.total_cg_files') AS INT) > 0\n",
    "    ORDER BY run_timestamp_utc DESC\n",
    "    LIMIT 10\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8543009708125849,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline_Runner_Modularized",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  },
  "microsoft": {
   "notebookName": "Pipeline_Runner_Modularized"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
